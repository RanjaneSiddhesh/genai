{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Generative AI Insem 2 - Codes with Problem Statements\n", "This notebook contains runnable codes along with their problem statements."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Basic Data Preprocessing for Generative AI\n", "**Problem Statement:** Generate synthetic data and scale it between 0 and 1 using Min-Max scaling."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Generate synthetic data\ndata = np.random.randint(0, 255, (10, 5))\nprint(\"Original Data:\\n\", data)\n\n# Scale data between 0 and 1\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(data)\nprint(\"Scaled Data:\\n\", scaled_data)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Visualizing Data Distributions\n", "**Problem Statement:** Generate synthetic data for two groups and visualize their distributions."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate synthetic data\ndata_group1 = np.random.normal(loc=50, scale=10, size=500)\ndata_group2 = np.random.normal(loc=200, scale=15, size=500)\n\n# Plot histogram\nplt.hist(data_group1, label='Group 1', color='blue', alpha=0.7)\nplt.hist(data_group2, label='Group 2', color='green', alpha=0.7)\nplt.title(\"Data Distribution\")\nplt.xlabel(\"Data Values\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. TensorFlow Computation Graph with Eager Execution\n", "**Problem Statement:** Perform computations in eager and graph execution modes."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import tensorflow as tf\n\na = tf.constant([5.0, 3.0])\nb = tf.constant([2.0, 7.0])\nc = a + b\nprint(\"Eager Execution Output:\", c.numpy())\n\n@tf.function\ndef multiply_tensors(x, y):\n    return x * y\n\nresult = multiply_tensors(a, b)\nprint(\"Graph Mode Output:\", result.numpy())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Word2Vec Embeddings\n", "**Problem Statement:** Train a Word2Vec model and find word similarities."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from gensim.models import Word2Vec\n\nsentences = [\n    [\"artificial\", \"intelligence\", \"is\", \"cool\"],\n    [\"machine\", \"learning\", \"is\", \"fun\"],\n    [\"ai\", \"learning\", \"uses\", \"neural\", \"networks\"]\n]\n\nmodel = Word2Vec(sentences, vector_size=10, window=2, min_count=1, sg=1)\n\nprint(\"Vector for 'learning':\", model.wv['learning'])\nprint(\"Most similar to 'learning':\", model.wv.most_similar('learning'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. GloVe Pre-trained Embeddings\n", "**Problem Statement:** Load GloVe embeddings and compute similarity."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import gensim.downloader as api\n\nglove_model = api.load(\"glove-wiki-gigaword-50\")\n\nprint(\"Vector for 'computer':\", glove_model['computer'])\nprint(\"Similarity between 'computer' and 'laptop':\", glove_model.similarity('computer', 'laptop'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6. BERT Embeddings\n", "**Problem Statement:** Extract embeddings using BERT."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from transformers import BertTokenizer, BertModel\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\ninputs = tokenizer(\"Generative AI creates realistic images\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nprint(\"BERT Output Shape:\", outputs.last_hidden_state.shape)\nprint(\"First token embedding:\", outputs.last_hidden_state[0][0][:5])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7. FAISS Similarity Search\n", "**Problem Statement:** Perform nearest neighbor search using FAISS."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import faiss\nimport numpy as np\n\ndata = np.random.random((5, 4)).astype('float32')\nindex = faiss.IndexFlatL2(4)\nindex.add(data)\n\nquery = np.random.random((1, 4)).astype('float32')\ndistances, indices = index.search(query, k=3)\n\nprint(\"Query Vector:\", query)\nprint(\"Top 3 Nearest Indices:\", indices)\nprint(\"Distances:\", distances)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8. Self-Attention Mechanism\n", "**Problem Statement:** Simulate self-attention using PyTorch."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\nimport torch.nn.functional as F\n\nx = torch.rand(1, 3, 4)  # (batch, seq_len, features)\nQ, K, V = x, x, x\n\nscores = torch.matmul(Q, K.transpose(-2, -1)) / (4 ** 0.5)\nweights = F.softmax(scores, dim=-1)\noutput = torch.matmul(weights, V)\n\nprint(\"Attention Weights:\", weights)\nprint(\"Output:\", output)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 9. Simulating Diffusion Denoising\n", "**Problem Statement:** Simulate iterative denoising in a diffusion process."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport matplotlib.pyplot as plt\n\nimage = np.random.rand(28, 28)\nplt.imshow(image, cmap='gray')\nplt.title(\"Step 0: Noise\")\nplt.show()\n\nfor step in range(1, 4):\n    image = image * 0.9  # reduce noise\n    plt.imshow(image, cmap='gray')\n    plt.title(f\"Step {step}: Denoising\")\n    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 10. FID Calculation\n", "**Problem Statement:** Compute the Fr\u00e9chet Inception Distance (FID) between two distributions."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from scipy.linalg import sqrtm\nimport numpy as np\n\ndef calculate_fid(mu1, sigma1, mu2, sigma2):\n    diff = mu1 - mu2\n    covmean = sqrtm(sigma1.dot(sigma2))\n    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n    return np.real(fid)\n\nmu1, sigma1 = np.random.rand(3), np.eye(3)\nmu2, sigma2 = np.random.rand(3), np.eye(3)\n\nprint(\"FID Score:\", calculate_fid(mu1, sigma1, mu2, sigma2))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}